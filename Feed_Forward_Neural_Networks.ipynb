{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feed Forward Neural Networks",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "fAgsLmKZMpyH"
      },
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Activation Functions"
      ],
      "metadata": {
        "id": "Mt8dmitqMs_4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {
        "id": "0lE5PJPWMcKJ"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1 + np.exp(-x))\n",
        "\n",
        "def d_sigmoid(x):\n",
        "  return (1 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1 - np.square(np.tanh(x))\n",
        "\n",
        "def relu(x):\n",
        "  return np.where(np.asarray(x) > 0, x, 0)\n",
        "\n",
        "def d_relu(x):\n",
        "    return np.where(x <= 0, 0, 1)\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x)\n",
        "    return e_x/e_x.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cost Function:"
      ],
      "metadata": {
        "id": "Yq4IZu4f2hCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(y, y_hat, i):\n",
        "  return -np.log(y_hat[y[i]][0])"
      ],
      "metadata": {
        "id": "2EE-A_Pw2kMi"
      },
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cost(y, y_hat, i):\n",
        "  \n",
        "  m = y.shape[0]\n",
        "  c = (1/m) * np.sum(cross_entropy_loss(y, y_hat))\n",
        "  c = np.squeeze(c) \n",
        "\n",
        "  return c"
      ],
      "metadata": {
        "id": "qJzVsQoi2yek"
      },
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Layer Class : parameters initialization for each layer"
      ],
      "metadata": {
        "id": "x23K52DJMwJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "\n",
        "    activationFunc = {\n",
        "        'tanh': (tanh, d_tanh),\n",
        "        'sigmoid': (sigmoid, d_sigmoid),\n",
        "        'relu' : (relu, d_relu),\n",
        "        'softmax' : (softmax, None)\n",
        "    }\n",
        "\n",
        "    def __init__(self, inputs, neurons, activation):\n",
        "        np.random.seed(33)\n",
        "        self.W = np.random.randn(neurons, inputs)\n",
        "        self.b = np.zeros((neurons, 1))\n",
        "        self.act, self.d_act = self.activationFunc.get(activation)\n",
        "        self.dW = 0\n",
        "        self.db = 0"
      ],
      "metadata": {
        "id": "MSYeILJNMvIV"
      },
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward propagation"
      ],
      "metadata": {
        "id": "pnulycSKMLB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(h, layers):\n",
        "  m = len(layers)\n",
        "  \n",
        "  layers[0].a = np.dot(layers[0].W, h)\n",
        "  layers[0].h = layers[0].act(layers[0].a)\n",
        "  #print(layers[0].h.shape)\n",
        "  \n",
        "  for j in range(1, m-1):\n",
        "    layers[j].a = np.dot(layers[j].W, layers[j-1].h)\n",
        "    layers[j].h = layers[j].act(layers[j].a)\n",
        "    #print(layers[j].h.shape)\n",
        "\n",
        "  j+=1\n",
        "  layers[j].a = np.dot(layers[j].W, layers[j-1].h)\n",
        "  layers[j].h = softmax(layers[j].a)\n",
        "  #print(layers[j].h.shape)\n",
        "\n",
        "  return layers[m-1].h"
      ],
      "metadata": {
        "id": "TK_UOVYHs3Vv"
      },
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Backward_propagation"
      ],
      "metadata": {
        "id": "9J1L5aJ3MNvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation(l, y_hat, layers, inp):\n",
        "  \n",
        "  #one-hot vector\n",
        "  e_l = np.zeros((y_hat.shape[0], 1))\n",
        "  e_l[l] = 1\n",
        "  \n",
        "  layers[len(layers)-1].da = -(e_l - y_hat)                 #gradient w.r.t activation of last layer (a_L)\n",
        "  \n",
        "  for j in range(len(layers)-1, 0, -1):\n",
        "                        \n",
        "    layers[j].dW += np.dot(layers[j].da, (layers[j-1].h).T)\n",
        "    layers[j].db += layers[j].da\n",
        "\n",
        "    layers[j-1].dh = np.dot((layers[j].W).T, layers[j].da)\n",
        "    layers[j-1].da = np.multiply(layers[j-1].dh, layers[j-1].d_act(layers[j-1].a))\n",
        "\n",
        "  layers[0].dW += np.dot(layers[0].da, inp.T)\n",
        "  layers[0].db += layers[0].da\n",
        "\n",
        "  return layers\n"
      ],
      "metadata": {
        "id": "Ib1ZyluaRYnz"
      },
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Gradient Descent"
      ],
      "metadata": {
        "id": "wSR_D3vzMR9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_params(learning_rate, layers, batch_size):\n",
        "  for layer in layers:\n",
        "    layer.W = layer.W - learning_rate * layer.dW/batch_size\n",
        "    layer.b = layer.b - learning_rate * layer.db/batch_size\n",
        "\n",
        "    layer.dW = 0\n",
        "    layer.db = 0"
      ],
      "metadata": {
        "id": "7ww8nMm7iVt8"
      },
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SGD / Batch Gradient Descent"
      ],
      "metadata": {
        "id": "h0Lo597P3hI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(epochs, layers, learning_rate, x_train, y_train, batch_size):\n",
        "    \n",
        "    m = x_train.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      cost = 0\n",
        "\n",
        "      for i in range(m):\n",
        "\n",
        "        inp = x_train[i].reshape(784, 1)\n",
        "        \n",
        "        # Feedforward\n",
        "        h = inp\n",
        "        h = forward_propagation(h, layers)\n",
        "\n",
        "        # Calulate cost to plot graph\n",
        "        cost += cross_entropy_loss(y_train, h, i)\n",
        "\n",
        "        # Backpropagation\n",
        "        backward_propagation(y_train[i], h, layers, x_train[i].reshape(784, 1))\n",
        "\n",
        "        #stocastic gradient decent\n",
        "        if (i+1) % batch_size == 0:\n",
        "          update_params(learning_rate, layers, batch_size)\n",
        "\n",
        "      costs.append(cost/m)\n",
        "\n",
        "      print(\"Cost after epoch \" + str(epoch) + \" :\", cost/m)\n",
        "\n",
        "    return costs, layers"
      ],
      "metadata": {
        "id": "Acn4nh9X3gIQ"
      },
      "execution_count": 328,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Momentum Gradient descent"
      ],
      "metadata": {
        "id": "ZcWfy6JBDyJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mgd(epochs, layers, learning_rate, x_train, y_train, batch_size):\n",
        "\n",
        "    gamma = 0.9\n",
        "    m = x_train.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      for layer in layers:\n",
        "        layer.update_W = 0\n",
        "        layer.update_b = 0\n",
        "\n",
        "      cost = 0\n",
        "\n",
        "      for i in range(m):\n",
        "\n",
        "        inp = x_train[i].reshape(784, 1)\n",
        "        \n",
        "        # Feedforward\n",
        "        h = inp\n",
        "        h = forward_propagation(h, layers)\n",
        "\n",
        "        # Calulate cost to plot graph\n",
        "        cost += cross_entropy_loss(y_train, h, i)\n",
        "\n",
        "        # Backpropagation\n",
        "        backward_propagation(y_train[i], h, layers, x_train[i].reshape(784, 1))\n",
        "\n",
        "        #momentum gradient decent\n",
        "        if (i+1) % batch_size == 0:\n",
        "          for layer in layers:\n",
        "\n",
        "            layer.update_W = gamma*layer.update_W + learning_rate*layer.dW/batch_size\n",
        "            layer.update_b = gamma*layer.update_b + learning_rate*layer.dW/batch_size\n",
        "\n",
        "            layer.W = layer.W - layer.update_W\n",
        "            layer.b = layer.b - layer.update_b\n",
        "\n",
        "            layer.dW = 0\n",
        "            layer.db = 0\n",
        "\n",
        "            layer.update_W = 0\n",
        "            layer.update_b = 0\n",
        "\n",
        "\n",
        "      costs.append(cost/m)\n",
        "\n",
        "      print(\"Cost after epoch \" + str(epoch) + \" :\", cost/m)\n",
        "\n",
        "    return costs, layers"
      ],
      "metadata": {
        "id": "vksSBdq-D1cC"
      },
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Nesterov Gradient Descent"
      ],
      "metadata": {
        "id": "Lp6A3a8gnoSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nesterov(epochs, layers, learning_rate, x_train, y_train, batch_size):\n",
        "\n",
        "    gamma = 0.9\n",
        "    m = x_train.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      for layer in layers:\n",
        "        layer.update_W = 0\n",
        "        layer.update_b = 0\n",
        "\n",
        "      cost = 0\n",
        "\n",
        "      for i in range(m):\n",
        "\n",
        "        inp = x_train[i].reshape(784, 1)\n",
        "        \n",
        "        # Feedforward\n",
        "        h = inp\n",
        "        h = forward_propagation(h, layers)\n",
        "\n",
        "        # Calulate cost to plot graph\n",
        "        cost += cross_entropy_loss(y_train, h, i)\n",
        "\n",
        "        #calculate W_lookaheads\n",
        "        if (i+1) % batch_size == 0:\n",
        "          for layer in layers:\n",
        "            layer.W = layer.W - gamma * layer.update_W\n",
        "            layer.b = layer.b - gamma * layer.update_b\n",
        "\n",
        "        # Backpropagation\n",
        "        backward_propagation(y_train[i], h, layers, x_train[i].reshape(784, 1))\n",
        "\n",
        "        #nesterov gradient decent\n",
        "        if (i+1) % batch_size == 0:\n",
        "          for layer in layers:\n",
        "\n",
        "            layer.update_W = gamma*layer.update_W + learning_rate*layer.dW/batch_size\n",
        "            layer.update_b = gamma*layer.update_b + learning_rate*layer.dW/batch_size\n",
        "\n",
        "            layer.W = layer.W - layer.update_W\n",
        "            layer.b = layer.b - layer.update_b\n",
        "\n",
        "            layer.dW = 0\n",
        "            layer.db = 0\n",
        "\n",
        "            layer.update_W = 0\n",
        "            layer.update_b = 0\n",
        "\n",
        "      costs.append(cost/m)\n",
        "      print(\"Cost after epoch \" + str(epoch) + \" :\", cost/m)\n",
        "      \n",
        "    return costs, layers  "
      ],
      "metadata": {
        "id": "dBdvwjl3nljy"
      },
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###RMSProp"
      ],
      "metadata": {
        "id": "pG_GGFWjXaRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsprop(epochs, layers, learning_rate, x_train, y_train, batch_size):\n",
        "\n",
        "    epsilon, beta = 1e-8, 0.9\n",
        "    m = x_train.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      for layer in layers:\n",
        "        layer.update_W = 0\n",
        "        layer.update_b = 0\n",
        "\n",
        "      cost = 0\n",
        "\n",
        "      for i in range(m):\n",
        "\n",
        "        inp = x_train[i].reshape(784, 1)\n",
        "        \n",
        "        # Feedforward\n",
        "        h = inp\n",
        "        h = forward_propagation(h, layers)\n",
        "\n",
        "        # Calulate cost to plot graph\n",
        "        cost += cross_entropy_loss(y_train, h, i)\n",
        "\n",
        "        # Backpropagation\n",
        "        backward_propagation(y_train[i], h, layers, x_train[i].reshape(784, 1))\n",
        "\n",
        "        #rmsprop gradient decent\n",
        "        if (i+1) % batch_size == 0:\n",
        "          for layer in layers:\n",
        "\n",
        "            layer.update_W = beta*layer.update_W + (1-beta)*(layer.dW/batch_size)**2\n",
        "            layer.update_b = beta*layer.update_b + (1-beta)*(layer.db/batch_size)**2\n",
        "\n",
        "            layer.W = layer.W - (learning_rate / np.sqrt(layer.update_W + epsilon)) * (layer.dW/batch_size)\n",
        "            layer.b = layer.b - (learning_rate / np.sqrt(layer.update_b + epsilon)) * (layer.db/batch_size)\n",
        "\n",
        "            layer.dW = 0\n",
        "            layer.db = 0\n",
        "\n",
        "            layer.update_W = 0\n",
        "            layer.update_b = 0\n",
        "\n",
        "\n",
        "      costs.append(cost/m)\n",
        "\n",
        "      print(\"Cost after epoch \" + str(epoch) + \" :\", cost/m)\n",
        "\n",
        "    return costs, layers"
      ],
      "metadata": {
        "id": "uGBnrjWeXcpt"
      },
      "execution_count": 331,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Adam"
      ],
      "metadata": {
        "id": "v0Dgl76nu7Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adam(epochs, layers, learning_rate, x_train, y_train, batch_size):\n",
        "\n",
        "    epsilon, beta1, beta2 = 1e-8, 0.9, 0.99\n",
        "    t = 0\n",
        "    \n",
        "    m = x_train.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      for layer in layers:\n",
        "        layer.m_W, layer.m_b, layer.v_W, layer.v_b, layer.m_W_hat, layer.m_b_hat, layer.v_W_hat, layer.v_b_hat = 0, 0, 0, 0, 0, 0, 0, 0\n",
        "\n",
        "      cost = 0\n",
        "\n",
        "      for i in range(m):\n",
        "\n",
        "        inp = x_train[i].reshape(784, 1)\n",
        "        \n",
        "        # Feedforward\n",
        "        h = inp\n",
        "        h = forward_propagation(h, layers)\n",
        "\n",
        "        # Calulate cost to plot graph\n",
        "        cost += cross_entropy_loss(y_train, h, i)\n",
        "\n",
        "        # Backpropagation\n",
        "        backward_propagation(y_train[i], h, layers, x_train[i].reshape(784, 1))\n",
        "\n",
        "        #adam gradient decent\n",
        "        if (i+1) % batch_size == 0:\n",
        "          t+=1\n",
        "\n",
        "          for layer in layers:\n",
        "\n",
        "            layer.m_W = beta1 * layer.m_W + (1-beta1)*layer.dW/batch_size\n",
        "            layer.m_b = beta1 * layer.m_b + (1-beta1)*layer.db/batch_size\n",
        "\n",
        "            layer.v_W = beta2 * layer.v_W + (1-beta2)*((layer.dW/batch_size))**2\n",
        "            layer.v_b = beta2 * layer.v_b + (1-beta2)*((layer.db/batch_size))**2\n",
        "\n",
        "            layer.m_W_hat = layer.m_W/(1-math.pow(beta1, t))\n",
        "            layer.m_b_hat = layer.m_b/(1-math.pow(beta1, t))\n",
        "\n",
        "            layer.v_W_hat = layer.v_W/(1-math.pow(beta2, t))\n",
        "            layer.v_b_hat = layer.v_b/(1-math.pow(beta2, t))\n",
        "\n",
        "            layer.W = layer.W - (learning_rate/np.sqrt(layer.v_W_hat + epsilon))*layer.m_W_hat\n",
        "            layer.b = layer.b - (learning_rate/np.sqrt(layer.v_b_hat + epsilon))*layer.m_b_hat\n",
        "\n",
        "            layer.dW = 0\n",
        "            layer.db = 0\n",
        "\n",
        "            layer.m_W, layer.m_b, layer.v_W, layer.v_b, layer.m_W_hat, layer.m_b_hat, layer.v_W_hat, layer.v_b_hat = 0, 0, 0, 0, 0, 0, 0, 0\n",
        "\n",
        "\n",
        "      costs.append(cost/m)\n",
        "\n",
        "      print(\"Cost after epoch \" + str(epoch) + \" :\", cost/m)\n",
        "\n",
        "    return costs, layers"
      ],
      "metadata": {
        "id": "OKZU2d9Su9Vk"
      },
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###NAdam"
      ],
      "metadata": {
        "id": "q9L5Efq9zs8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nadam(epochs, layers, learning_rate, x_train, y_train, batch_size):\n",
        "\n",
        "    epsilon, beta1, beta2 = 1e-8, 0.9, 0.99\n",
        "    gamma = 0.9\n",
        "    t = 0\n",
        "    \n",
        "    m = x_train.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      for layer in layers:\n",
        "        layer.m_W, layer.m_b, layer.v_W, layer.v_b, layer.m_W_hat, layer.m_b_hat, layer.v_W_hat, layer.v_b_hat = 0, 0, 0, 0, 0, 0, 0, 0\n",
        "        layer.update_W = 0\n",
        "        layer.update_b = 0\n",
        "\n",
        "      cost = 0\n",
        "\n",
        "      for i in range(m):\n",
        "\n",
        "        inp = x_train[i].reshape(784, 1)\n",
        "        \n",
        "        # Feedforward\n",
        "        h = inp\n",
        "        h = forward_propagation(h, layers)\n",
        "\n",
        "        # Calulate cost to plot graph\n",
        "        cost += cross_entropy_loss(y_train, h, i)\n",
        "\n",
        "        #calculate W_lookaheads\n",
        "        if (i+1) % batch_size == 0:\n",
        "          for layer in layers:\n",
        "            layer.W = layer.W - gamma * layer.m_W\n",
        "            layer.b = layer.b - gamma * layer.m_b\n",
        "        \n",
        "        # Backpropagation\n",
        "        backward_propagation(y_train[i], h, layers, x_train[i].reshape(784, 1))\n",
        "\n",
        "        #adam gradient decent\n",
        "        if (i+1) % batch_size == 0:\n",
        "          t+=1\n",
        "\n",
        "          for layer in layers:\n",
        "\n",
        "            layer.m_W = beta1 * layer.m_W + (1-beta1)*layer.dW/batch_size\n",
        "            layer.m_b = beta1 * layer.m_b + (1-beta1)*layer.db/batch_size\n",
        "\n",
        "            layer.v_W = beta2 * layer.v_W + (1-beta2)*((layer.dW/batch_size))**2\n",
        "            layer.v_b = beta2 * layer.v_b + (1-beta2)*((layer.db/batch_size))**2\n",
        "\n",
        "            layer.m_W_hat = layer.m_W/(1-math.pow(beta1, t))\n",
        "            layer.m_b_hat = layer.m_b/(1-math.pow(beta1, t))\n",
        "\n",
        "            layer.v_W_hat = layer.v_W/(1-math.pow(beta2, t))\n",
        "            layer.v_b_hat = layer.v_b/(1-math.pow(beta2, t))\n",
        "\n",
        "            layer.m_dash_W = beta1 * layer.m_W_hat + (1-beta1)*layer.dW/batch_size\n",
        "            layer.m_dash_b = beta1 * layer.m_b_hat + (1-beta1)*layer.db/batch_size\n",
        "\n",
        "            layer.W = layer.W - (learning_rate/np.sqrt(layer.v_W_hat + epsilon))*layer.m_dash_W\n",
        "            layer.b = layer.b - (learning_rate/np.sqrt(layer.v_b_hat + epsilon))*layer.m_dash_b\n",
        "\n",
        "            layer.dW = 0\n",
        "            layer.db = 0\n",
        "\n",
        "            layer.m_W, layer.m_b, layer.v_W, layer.v_b, layer.m_W_hat, layer.m_b_hat, layer.v_W_hat, layer.v_b_hat = 0, 0, 0, 0, 0, 0, 0, 0\n",
        "\n",
        "\n",
        "      costs.append(cost/m)\n",
        "\n",
        "      print(\"Cost after epoch \" + str(epoch) + \" :\", cost/m)\n",
        "\n",
        "    return costs, layers"
      ],
      "metadata": {
        "id": "QdwVLM1fzu7V"
      },
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Putting all togather:"
      ],
      "metadata": {
        "id": "os37IrNr-ClW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Function to Train Model"
      ],
      "metadata": {
        "id": "BN3FKWWGNkN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_train(optimizer, epochs, learning_rate, x_train, y_train, activation, h_layers, neurons, batch_size):\n",
        "\n",
        "  layers= [Layer(x_train.shape[1], neurons, activation)]\n",
        "\n",
        "  for _ in range(0, h_layers-1):\n",
        "    layers.append(Layer(neurons, neurons, activation))\n",
        "  layers.append(Layer(neurons, 10, 'softmax'))\n",
        "  \n",
        "  if optimizer == \"sgd\":\n",
        "    return sgd(epochs, layers, learning_rate, x_train, y_train, batch_size)\n",
        "  elif optimizer == \"mgd\":\n",
        "    return mgd(epochs, layers, learning_rate, x_train, y_train, batch_size)\n",
        "  elif optimizer == \"nesterov\":\n",
        "    return nesterov(epochs, layers, learning_rate, x_train, y_train, batch_size)\n",
        "  elif optimizer == \"rmsprop\":\n",
        "    return rmsprop(epochs, layers, learning_rate, x_train, y_train, batch_size)\n",
        "  elif optimizer == \"adam\":\n",
        "    return adam(epochs, layers, learning_rate, x_train, y_train, batch_size)\n",
        "  elif optimizer == \"nadam\":\n",
        "    return nadam(epochs, layers, learning_rate, x_train, y_train, batch_size)\n",
        "  else:\n",
        "    print(\"No optimization algorithm named \"+optimizer+\" found\")\n",
        "    return \"Error\", \"Error\""
      ],
      "metadata": {
        "id": "3sRz9nk6NgVa"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Function to Predict"
      ],
      "metadata": {
        "id": "jbjCtQVxNYwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(input, y):\n",
        "  prediction = forward_propagation(input, layers)\n",
        "  prediction = prediction.argmax(axis=0)\n",
        "  accuracy =  np.sum(prediction == y)/y.shape[0]\n",
        "  return prediction, accuracy"
      ],
      "metadata": {
        "id": "LXkMp1rLNbaj"
      },
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import dataset and putting in appropriate format"
      ],
      "metadata": {
        "id": "VcVmssSHNp78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(x_train_org, y_train_org), (x_test_org, y_test_org) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "-gFb9PXW50Fc"
      },
      "execution_count": 336,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"x_train shape: \", x_train_org.shape)\n",
        "print(\"y_train shape: \", y_train_org.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxKqgiBfFg0R",
        "outputId": "774b0200-de93-4ff4-d007-e541e4554c61"
      },
      "execution_count": 337,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape:  (60000, 28, 28)\n",
            "y_train shape:  (60000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''first_image = x_train_org[0]\n",
        "#first_image = np.array(first_image, dtype='float')\n",
        "#pixels = first_image.reshape((28, 28))\n",
        "i = True\n",
        "while(i):\n",
        "  if\n",
        "  plt.imshow(x_train_org[i], cmap='gray')\n",
        "  plt.title(\"class: \"+ str(y_train_org[i]))\n",
        "  plt.show()'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "wAlbklIDXAax",
        "outputId": "712a680e-91aa-4a53-815d-cd005d5339a3"
      },
      "execution_count": 338,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'first_image = x_train_org[0]\\n#first_image = np.array(first_image, dtype=\\'float\\')\\n#pixels = first_image.reshape((28, 28))\\ni = True\\nwhile(i):\\n  if\\n  plt.imshow(x_train_org[i], cmap=\\'gray\\')\\n  plt.title(\"class: \"+ str(y_train_org[i]))\\n  plt.show()'"
            ]
          },
          "metadata": {},
          "execution_count": 338
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Flattening the data"
      ],
      "metadata": {
        "id": "L9eUrqrAU1N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_temp = x_train_org.reshape(x_train_org.shape[0], -1)\n",
        "y_train_temp = y_train_org\n",
        "x_test = x_test_org.reshape(x_test_org.shape[0], -1)\n",
        "y_test = y_test_org"
      ],
      "metadata": {
        "id": "aVrG8aF6A6HY"
      },
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Splliting dataset into training and validation"
      ],
      "metadata": {
        "id": "ykV4aRkBU67b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(x_train_temp, y_train_temp, test_size=0.1, random_state=33)"
      ],
      "metadata": {
        "id": "QdiEOLK1TryC"
      },
      "execution_count": 340,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"x_train shape: \", x_train.shape)\n",
        "print(\"y_train shape: \", y_train.shape)\n",
        "print(\"x_val shape: \", x_val.shape)\n",
        "print(\"y_val shape: \", y_val.shape)\n",
        "print(\"x_test shape: \", x_test.shape)\n",
        "print(\"y_test shape: \", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQzSXM6cJn2H",
        "outputId": "850d5b52-7f61-4ff0-b668-f437a4f51b88"
      },
      "execution_count": 341,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape:  (54000, 784)\n",
            "y_train shape:  (54000,)\n",
            "x_val shape:  (6000, 784)\n",
            "y_val shape:  (6000,)\n",
            "x_test shape:  (10000, 784)\n",
            "y_test shape:  (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train Model"
      ],
      "metadata": {
        "id": "Wfm6cD7aM9nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = \"nadam\"\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "activation = 'tanh'\n",
        "h_layers = 5\n",
        "neurons = 64\n",
        "batch_size = 16\n",
        "\n",
        "costs, layers = model_train(optimizer, epochs, learning_rate, x_train, y_train, activation, h_layers, neurons, batch_size)"
      ],
      "metadata": {
        "id": "hrKrKnZSKxYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f0b4ea7-d39a-4d6b-fb9c-4f98fa15242d"
      },
      "execution_count": 342,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after epoch 0 : 3.441106471573012\n",
            "Cost after epoch 1 : 1.486003595629301\n",
            "Cost after epoch 2 : 1.264526580205672\n",
            "Cost after epoch 3 : 1.158255766414733\n",
            "Cost after epoch 4 : 1.102467952098729\n",
            "Cost after epoch 5 : 1.0276968541390494\n",
            "Cost after epoch 6 : 1.0119884944787592\n",
            "Cost after epoch 7 : 1.0084171028801403\n",
            "Cost after epoch 8 : 0.9957740960703261\n",
            "Cost after epoch 9 : 0.9881838511667896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cost per epoch graph"
      ],
      "metadata": {
        "id": "o7X4G_o3M0ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(epochs), costs)\n",
        "print(costs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "jf123LkjMz_z",
        "outputId": "eeb12c68-5db1-4657-870a-8cf33892216b"
      },
      "execution_count": 343,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3.441106471573012, 1.486003595629301, 1.264526580205672, 1.158255766414733, 1.102467952098729, 1.0276968541390494, 1.0119884944787592, 1.0084171028801403, 0.9957740960703261, 0.9881838511667896]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAa0ElEQVR4nO3de3Cdd53f8ff3XHS/HFlWbNk6iu2QOHEulsBJHLJluaUbLg3TXbYlnULJLM2ywEIonZ3CTNlZZjqdztB0uezCZiFc2pSyC5RJaWAJkOXW2KDYTkx8CY6T+G7Lsi3JlnU5Ot/+8TySJUWOjuwjP+d5zuc1c0bPeZ6fzvnOmeSjx7/n93yPuTsiIhJ/qagLEBGR8lCgi4gkhAJdRCQhFOgiIgmhQBcRSYhMVG+8fPlyX7NmTVRvLyISS0899dRJd++Y71hkgb5mzRr6+vqiensRkVgys5cudmzBKRczqzOzX5nZ02b2rJn9xTxj3mtm/Wa2I3y873KLFhGRxSnlDH0MeKO7nzWzLPALM/u+u2+ZM+6b7v6h8pcoIiKlWDDQPbiV9Gz4NBs+dHupiEiFKWmVi5mlzWwHcAJ43N23zjPsD8zsGTP7lpnlL/I695tZn5n19ff3X0bZIiIyV0mB7u6T7t4DdAG3mdlNc4b8H2CNu98CPA587SKv85C7b3L3TR0d816kFRGRS7SodejufgZ4Arh7zv4Bdx8Ln34JeE15yhMRkVKVssqlw8xy4XY9cBewZ86YzhlP7wF2l7NIERFZWCln6J3AE2b2DPBrgjn075nZp8zsnnDMh8MljU8DHwbeuzTlwt5jw/yn/7uLkfHCUr2FiEgslbLK5Rmgd579n5yx/XHg4+UtbX6HTo/wtz9/gbs2rOS2tcuuxFuKiMRC7Hq59ORzAOw4eDriSkREKkvsAr29qZb8snp2HDwTdSkiIhUldoEO0JNvY8cBBbqIyEwxDfQcRwZHOTE0GnUpIiIVI6aB3grAdk27iIhMi2Wg37iqlUzKNI8uIjJDLAO9Lpvmhs4WzaOLiMwQy0CHYB79mUNnmCyq8aOICMQ80M+NT7LvxNmFB4uIVIH4Bnq3bjASEZkptoG+tr2RlrqMLoyKiIRiG+iplLExn2PHwcGoSxERqQixDXSA3nyOvceG1HlRRISYB3pPd46iw85DOksXEYl1oG/smrowqnl0EZFYB3p7Uy3dyxoU6CIixDzQIViPrkAXEUlIoB8dHOW4Oi+KSJWLf6CHNxhtV18XEalysQ/0DZ0tZNPqvCgiEvtAr8um2dDZohYAIlL1Yh/oEMyj7zw0qM6LIlLVkhHo3UHnxd+eGI66FBGRyCQj0PNtAPrCCxGpaokI9DXtDbTWZ3VhVESqWiIC3Wyq86ICXUSqVyICHYILo88dH+bcmDovikh1Skyg9+aDzovPqPOiiFSpxAT6xrw6L4pIdUtMoC9rrOHq9gbdYCQiVSsxgQ7BPPrT+ko6EalSiQv0Y0OjHBtU50URqT4LBrqZ1ZnZr8zsaTN71sz+Yp4xtWb2TTPbZ2ZbzWzNUhS7kJ7peXRNu4hI9SnlDH0MeKO7bwR6gLvNbPOcMX8EnHb3VwH/Dfgv5S2zNBtWtVCTTrFdF0ZFpAotGOgeOBs+zYaPuV2w3gF8Ldz+FvAmM7OyVVmi2kyaG1a1qAWAiFSlkubQzSxtZjuAE8Dj7r51zpDVwEEAdy8Ag0D7PK9zv5n1mVlff3//5VV+Eb35HDsPq/OiiFSfkgLd3SfdvQfoAm4zs5su5c3c/SF33+Tumzo6Oi7lJRbUk88xMj7Jc8fVeVFEqsuiVrm4+xngCeDuOYcOA3kAM8sArcBAOQpcrB7dYCQiVaqUVS4dZpYLt+uBu4A9c4Y9CvybcPudwE/cPZI5j6vbG2hryGoeXUSqTqaEMZ3A18wsTfAH4O/c/Xtm9imgz90fBb4M/Hcz2wecAt61ZBUvQJ0XRaRaLRjo7v4M0DvP/k/O2B4F/rC8pV26nnyOnz73W86OFWiqLeVvlohI/CXqTtEpPfkc7vDMIZ2li0j1SGyggy6Mikh1SWSg5xpqWLu8URdGRaSqJDLQATZ2tbLj4BkiWmwjInLFJTbQe/I5TgyPcVSdF0WkSiQ30LvbAM2ji0j1SGyg39DZTE06pUAXkaqR2ECvzaTZoM6LIlJFEhvoEMyj7zw8SGGyGHUpIiJLLtGB3tud4/zEJM8dP7vwYBGRmEt0oOsGIxGpJokO9O5lDSxrrNF3jIpIVUh0oJvZ9A1GIiJJl+hAB+jJt/HbE2cZHp2IuhQRkSWV/EDvDjov7jw0GHUpIiJLKvmB3hVcGN2uaRcRSbjEB3prQ5Z1yxs1jy4iiZf4QIdg+aI6L4pI0lVHoHfn6B8e44g6L4pIglVHoE/dYKS+LiKSYFUR6NevbKEmk9INRiKSaFUR6DWZFDetatGFURFJtKoIdAhuMNp5eJAJdV4UkYSqnkDvzjE6UWTvseGoSxERWRLVE+hd6rwoIslWNYGeX1Yfdl5UoItIMlVNoJvZ9A1GIiJJVDWBDsF69Of7zzKkzosikkBVF+jqvCgiSVVVgb5RX0knIglWVYHeWp9lXUcj29UCQEQSaMFAN7O8mT1hZrvM7Fkz+8g8Y15vZoNmtiN8fHJpyr186rwoIklVyhl6AfiYu28ANgMfNLMN84z7ubv3hI9PlbXKMurN5zh5dozDZ85HXYqISFktGOjuftTdt4Xbw8BuYPVSF7ZUevJtgObRRSR5FjWHbmZrgF5g6zyH7zCzp83s+2Z240V+/34z6zOzvv7+/kUXWw7XdzZTm0mpla6IJE7JgW5mTcC3gQfcfWjO4W3A1e6+Efgc8N35XsPdH3L3Te6+qaOj41JrvizZdIqbVrfqDF1EEqekQDezLEGYP+Lu35l73N2H3P1suP0YkDWz5WWttIx68jl1XhSRxClllYsBXwZ2u/uDFxmzMhyHmd0Wvu5AOQstp558jrGCOi+KSLJkShhzJ/BuYKeZ7Qj3fQLoBnD3LwLvBP7EzArAeeBdXsHrAqe+km77wTPctLo14mpERMpjwUB3918AtsCYzwOfL1dRS62rrZ7lTTXsOHCGd2++OupyRETKoqruFJ1yofOivmNURJKjKgMdpjovnmPwvDovikgyVHGgBzcYPXNIyxdFJBmqNtBvyQcXQ3WDkYgkRdUGektdlms6GnWDkYgkRtUGOgTTLuq8KCJJUd2B3p1j4Nw4h06r86KIxF9VB3qvvsFIRBKkqgN9/cqw86ICXUQSoKoDPZtOcbM6L4pIQlR1oENwg9Fv1HlRRBJAgd4ddF7cc1SdF0Uk3hTo0xdG1ddFROKt6gN9da6e5U21bNc8uojEXNUH+oXOiwp0EYm3qg90gN7uHPv7zzE4os6LIhJfCnQuzKM/rc6LIhJjCnTglq5WzHTHqIjEmwIdaK7L8qqOJgW6iMSaAj00dWFUnRdFJK4U6KGe7hynzo1z8JQ6L4pIPCnQQ1MXRrfrBiMRiSkFemj9imbqs2nNo4tIbCnQQxl1XhSRmFOgz7Ax38qzR4YYL6jzoojEjwJ9hp58G+OFIruPDkVdiojIoinQZ+jp1h2jIhJfCvQZVrXW0dFcy44DCnQRiR8F+gzqvCgicaZAn6Mnn2P/SXVeFJH4UaDP0Tv1DUaaRxeRmFkw0M0sb2ZPmNkuM3vWzD4yzxgzs8+a2T4ze8bMXr005S69m6c6L2oeXURiJlPCmALwMXffZmbNwFNm9ri775ox5i3AteHjduAL4c/Yaa7Lcu1VTfqOURGJnQXP0N39qLtvC7eHgd3A6jnD3gF83QNbgJyZdZa92itEnRdFJI4WNYduZmuAXmDrnEOrgYMznh/i5aGPmd1vZn1m1tff37+4Sq+gnnwbp0cmOHBqJOpSRERKVnKgm1kT8G3gAXe/pFsp3f0hd9/k7ps6Ojou5SWuiKnOi1q+KCJxUlKgm1mWIMwfcffvzDPkMJCf8bwr3BdL161ooj6bZrsujIpIjJSyysWALwO73f3Biwx7FHhPuNplMzDo7kfLWOcVlUmnuLlLnRdFJF5KWeVyJ/BuYKeZ7Qj3fQLoBnD3LwKPAW8F9gEjwH3lL/XK6s3n+MovX2SsMEltJh11OSIiC1ow0N39F4AtMMaBD5arqErQk88xPllk99Hh6Tl1EZFKpjtFL2Kq8+KOA1qPLiLxoEC/iM7Wela01GoeXURiQ4H+CtR5UUTiRIH+Cnrybbw4MMLpc+NRlyIisiAF+ivYmG8F1HlRROJBgf4KbunKYQZPa9pFRGJAgf4KmmozXHdVs+bRRSQWFOgL6MnneFqdF0UkBhToC+jpznF6ZIKXBtR5UUQqmwJ9Aeq8KCJxoUBfwHUrmmmoSSvQRaTiKdAXkE4ZN69uZbsCXUQqnAK9BD3dOXYfGWKsMBl1KSIiF6VAL0Fv2Hlx15FL+qImEZErQoFegp58G6ALoyJS2RToJVjZWsfKljoFuohUNAV6idR5UUQqnQK9RD3dOV4aGOGUOi+KSIVSoJdo6gYjNeoSkUqlQC/RzatbSRlajy4iFUuBXqLG2gzXrVDnRRGpXAr0RejtVudFEalcCvRF6MnnGDw/wQsnz0VdiojIyyjQF0E3GIlIJVOgL8KrrmqisSatlS4iUpEU6IuQThk3d7XqDF1EKpICfZF68m3sOjrE6IQ6L4pIZVGgL1JPPsfEpLPrqDovikhlUaAvUm93+JV0BzTtIiKVRYG+SCta6uhsVedFEak8CvRLoM6LIlKJFOiXoCef48CpEQbOjkVdiojItAUD3cweNrMTZvabixx/vZkNmtmO8PHJ8pdZWaY7Lx7SWbqIVI5SztC/Cty9wJifu3tP+PjU5ZdV2W7uaiWdMl0YFZGKsmCgu/vPgFNXoJbYaKgJOi+qla6IVJJyzaHfYWZPm9n3zezGiw0ys/vNrM/M+vr7+8v01tHoyefoe/E0f/frgxQmi1GXIyJSlkDfBlzt7huBzwHfvdhAd3/I3Te5+6aOjo4yvHV03v+767huZTN/9u1n+L2//BmP7TyqtroiEqnLDnR3H3L3s+H2Y0DWzJZfdmUV7ur2Rr77gdfyN+9+DSkzPvDINu75/C/52XP9CnYRicRlB7qZrTQzC7dvC19z4HJfNw7MjN+7cSU/eOB1/Nc/3Mipc+O85+Ff8a/+divbDpyOujwRqTKZhQaY2TeA1wPLzewQ8OdAFsDdvwi8E/gTMysA54F3eZWdoqZTxh+8pou3b+zkG1sP8Pkn9vH7f/3/uGvDCv79P13P+pXNUZcoIlXAosreTZs2eV9fXyTvvdTOjRX4yi9f4G9+up+z4wX+ec9qPnrXdeSXNURdmojEnJk95e6b5j2mQF86Z0bG+cJPn+erv3yRojv33tbNh974Kq5qrou6NBGJKQV6xI4PjfLZH/+Wb/76INl0ivvuXMMf/+41tNZnoy5NRGJGgV4hXjx5jgcff45Hnz5CS12G97/+Gu577Vrqa9JRlyYiMaFArzC7jgzx6R/u5Sd7TtDRXMuH33Qt/3JTnpqMeqWJyCt7pUBXgkRgw6oWHn7vrfz9++9gbXsj//G7v+HND/6U724/TLFYVQuERKSMFOgRunXNMr75x5v5yn230lSb4YFv7uCtn/05P9p1XDcniciiKdAjZma8Yf1VfO9Pf4fP3dvLWKHI+77exzu/+CRb9lfF/VkiUiYK9AqRShn/bOMqfvjR1/Gff/9mDp8+z7se2sJ7Hv4Vvzk8GHV5IhIDuihaoUYnJvn6ky/y1//4PGdGJnjbLZ38u7uu45qOpqhLE5EIaZVLjA2NTvCln+3nS794gbFCkXe+uouPvPlaVuXqoy5NRCKgQE+Ak2fH+Ksn9vHIlgNg8J7NV/OBN7yKZY01UZcmIleQAj1BDp0e4TM/+i3f3naIhpoM7/sna/mj31lLc53uOhWpBgr0BNp3YphP/8Nz/ODZY2TTxsauHHdc084d69p59dVt1GV196lIEinQE2znoUG+t/MIW54fYOfhQYoONekUvd05Nq9r545r2untzlGbUcCLJIECvUoMjU7Q9+Ipnnx+gCf3D/DskSHcoTaT4jVXt3FHGPC3dOXUZkAkphToVWpwZIKtLwywZf8pntw/wO6jQwDUZ9NsWtM2fQZ/8+pWsmkFvEgcKNAFgNPnxtn6wgBPPh+E/N7jwwA01qS5de2yIODXtXPjqhYyCniRivRKgb7gV9BJcrQ11nD3TZ3cfVMnECyF3Lr/FE/uP8mTzw/wj3v7AWiuzXDb2mXccU07m9e1c0NnC+mURVm6iJRAgV7FljfV8rZbOnnbLUHAnxgeDaZnnh9gy/4BfrznBACt9dkg4MMpmvUrmkkp4EUqjgJdpl3VXMc9G1dxz8ZVABwbHOXJ/SfZ8nwwB//4ruMAtDVkuX1tEO53XNPOtVc1YaaAF4ma5tClZIfPnA9W0IRn8IfPnAdgeVMNt69r56ZVrXQva5h+tDboZieRctNFUSk7d+fQ6fPTSyS37B/g6ODorDEtdRm624Nwz88I+u5lDazK1Wtljcgl0EVRKTszIx8G9b+4NQ/A2bECB0+NcODUCAdPjfDSQLC95+gwP9p1gvHJ4vTvpwxW5eovhHz77MBvrc9qGkdkkRToUjZNtRlu6Gzhhs6Wlx2bLDrHh0Y5MCPwp7Z/tPs4J8+OzxrfXJeZFfD5OWf3ujFK5OUU6HJFpFPGqlw9q3L1bF7X/rLj58YKHDw9woGB2YH/3PFhfrznBOOF2Wf3na2zz+5nBr46UEq1UqBLRWiszXD9yhauX/nys/ti0Tk+PPqysD9waoQf7znBybNjs8a3N9ZwfWcz61e0cH1nMzesbOHaFU1qWCaJp0CXipdKGZ2t9XS21nP7PGf3I+MFDp46z4FTI7w0cI7njg+z59gw//NXLzE6EZzZpwzWLG/k+pXN4R+O4GdXW73W1EtiKNAl9hpqMqxf2cz6lc2z9k8WnZcGzrH32DC7jw2z5+gQzx4Z4rGdx6bHNNakuS4M9xs6m1m/ItjWkkuJIy1blKpzbqwwfRa/99gwu48OsefYMIPnJ6bHrGqtY/3KZq7vvHA2v66jUUstJXJatigyQ2Ntht7uNnq726b3uTvHh8bYfWyIveHZ/J5jw/xi30kmJoOTnmzauKajiRs6W4KwX9nMDZ0tXNVcqyWWUhEU6CIE6+pXttaxsrWON6y/anr/eKHI/pNnwzP5YfYeG2LL/gH+9/bD02NyDdnZc/OdLVx7VRMNNWkFvVxRCwa6mT0MvB044e43zXPcgM8AbwVGgPe6+7ZyFyoShZpManr1zTt6LuwfHJlgz7HgLD54DPH3fQc5Nz45PSadMuqzaeqyKeqyaeqyaerDR202FR4L99XMs2/uuJr0vK9Xl02rG6YApZ2hfxX4PPD1ixx/C3Bt+Lgd+EL4UySxWhuy3L6ufdaqm2IxaIew+9gQz/efZWRskvMTwWN0+lHk/Pgkw6MF+ofHLuwLj4/NWG+/GDXp1HTQTwV/bTZNbSZF2oxUClJmmBlpu7CdCrenjqdm7Jt9fMa2ER4z0ql5xhrh+AtjMymjNpOiJhPUVJtNUZtJU5NJBc8zqXB7xvF08AetJp3SSqQSLRjo7v4zM1vzCkPeAXzdg6urW8wsZ2ad7n60TDWKxEIqZUELg/aGS36NYtEZLcwO+fPjk4wVJjk/PmPfxCRj4c/z40VGCzPHXfj9scIkk+4UCs5k0Sl6cL2g6FD0YJ+H20W/sD3pTrE4e2xx5nb4e5Mz9s8cW+61Ftm0XQj7meEfBv7UH4gLx+Y+D8Zm0ykyKSOTDv7IZFKpcDv4mU0b6VSKbMrIpFOkU1P77MLvTv1OeubvB9vZtEU6zVaOOfTVwMEZzw+F+14W6GZ2P3A/QHd3dxneWiRZUimjoSZDQ8xvdp37h6Aw6YwXiowViowVJmdtj00UGZssBj9nHZvzfKLI+GQ4fs6x0YkiQ+cLwetNjy0yFv6rp1C8cqv5UgaZdPBHYeoPwfQfhPCPw723dvNvX7eu7O99RS+KuvtDwEMQLFu8ku8tIlfO1NROmuBstTYDjbXR1TNZdMYKk0xMOoXJIOALxWB7YjL4l8rEZJHJolMozt5XmAzHFovhvhmvMee1gp/ORLHIZPh7U68bvGaRiaLT0bw0H0Y5Av0wkJ/xvCvcJyJSEdLhv3ySrhx3STwKvMcCm4FBzZ+LiFx5pSxb/AbwemC5mR0C/hzIArj7F4HHCJYs7iNYtnjfUhUrIiIXV8oql3sXOO7AB8tWkYiIXBI1phARSQgFuohIQijQRUQSQoEuIpIQCnQRkYSI7AsuzKwfeOkSf305cLKM5cSdPo/Z9HlcoM9itiR8Hle7e8d8ByIL9MthZn0X+8aOaqTPYzZ9Hhfos5gt6Z+HplxERBJCgS4ikhBxDfSHoi6gwujzmE2fxwX6LGZL9OcRyzl0ERF5ubieoYuIyBwKdBGRhIhdoJvZ3Wa218z2mdl/iLqeKJlZ3syeMLNdZvasmX0k6pqiZmZpM9tuZt+Lupaohd/v+y0z22Nmu83sjqhrioqZfTT8f+Q3ZvYNM6uLuqalEKtAN7M08FfAW4ANwL1mtiHaqiJVAD7m7huAzcAHq/zzAPgIsDvqIirEZ4AfuPv1wEaq9HMxs9XAh4FN7n4TkAbeFW1VSyNWgQ7cBuxz9/3uPg78L+AdEdcUGXc/6u7bwu1hgv9hV0dbVXTMrAt4G/ClqGuJmpm1Aq8Dvgzg7uPufibaqiKVAerNLAM0AEcirmdJxC3QVwMHZzw/RBUH2ExmtgboBbZGW0mk/hL4M6AYdSEVYC3QD3wlnIL6kpk1Rl1UFNz9MPBp4ABwlOBrMn8YbVVLI26BLvMwsybg28AD7j4UdT1RMLO3Ayfc/amoa6kQGeDVwBfcvRc4B1TlNSczayP4l/xaYBXQaGb/OtqqlkbcAv0wkJ/xvCvcV7XMLEsQ5o+4+3eiridCdwL3mNmLBFNxbzSz/xFtSZE6BBxy96l/sX2LIOCr0ZuBF9y9390ngO8Ar424piURt0D/NXCtma01sxqCCxuPRlxTZMzMCOZId7v7g1HXEyV3/7i7d7n7GoL/Ln7i7ok8CyuFux8DDprZ+nDXm4BdEZYUpQPAZjNrCP+feRMJvUC84JdEVxJ3L5jZh4B/ILhS/bC7PxtxWVG6E3g3sNPMdoT7PuHuj0VYk1SOPwUeCU9+9gP3RVxPJNx9q5l9C9hGsDJsOwltAaBb/0VEEiJuUy4iInIRCnQRkYRQoIuIJIQCXUQkIRToIiIJoUAXEUkIBbqISEL8f0yvQEjDAT7jAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Predictions and accuracy using validation data and test data"
      ],
      "metadata": {
        "id": "tuGIytQQLYHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_val, accuracy_val = predict(x_val.T, y_val)\n",
        "output_test, accuracy_test = predict(x_test.T, y_test)\n",
        "print(\"Validation accuracy: \", accuracy_val)\n",
        "print(\"Test accuracy: \", accuracy_test)"
      ],
      "metadata": {
        "id": "cV2-vMJ4igo4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2494a8c6-fc1f-4ca0-ccae-fa26355d4e68"
      },
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy:  0.6718333333333333\n",
            "Test accuracy:  0.6686\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feed Forward Neural Networks",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "metadata": {
        "id": "fAgsLmKZMpyH"
      },
      "execution_count": 344,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Activation Functions"
      ],
      "metadata": {
        "id": "Mt8dmitqMs_4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 345,
      "metadata": {
        "id": "0lE5PJPWMcKJ"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1 + np.exp(-x))\n",
        "\n",
        "def d_sigmoid(x):\n",
        "  return (1 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1 - np.square(np.tanh(x))\n",
        "\n",
        "def relu(x):\n",
        "  return np.where(np.asarray(x) > 0, x, 0)\n",
        "\n",
        "def d_relu(x):\n",
        "    return np.where(x <= 0, 0, 1)\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x)\n",
        "    return e_x/e_x.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cost Function:"
      ],
      "metadata": {
        "id": "Yq4IZu4f2hCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(y, y_hat, i):\n",
        "  return -np.log(y_hat[y[i]][0])"
      ],
      "metadata": {
        "id": "2EE-A_Pw2kMi"
      },
      "execution_count": 346,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cost(y, y_hat, i):\n",
        "  \n",
        "  m = y.shape[0]\n",
        "  c = (1/m) * np.sum(cross_entropy_loss(y, y_hat))\n",
        "  c = np.squeeze(c) \n",
        "\n",
        "  return c"
      ],
      "metadata": {
        "id": "qJzVsQoi2yek"
      },
      "execution_count": 347,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Layer Class : parameters initialization for each layer"
      ],
      "metadata": {
        "id": "x23K52DJMwJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "\n",
        "    activationFunc = {\n",
        "        'tanh': (tanh, d_tanh),\n",
        "        'sigmoid': (sigmoid, d_sigmoid),\n",
        "        'relu' : (relu, d_relu),\n",
        "        'softmax' : (softmax, None)\n",
        "    }\n",
        "\n",
        "    def __init__(self, inputs, neurons, activation):\n",
        "        np.random.seed(33)\n",
        "        self.W = np.random.randn(neurons, inputs)\n",
        "        self.b = np.zeros((neurons, 1))\n",
        "        self.act, self.d_act = self.activationFunc.get(activation)\n",
        "        self.dW = 0\n",
        "        self.db = 0"
      ],
      "metadata": {
        "id": "MSYeILJNMvIV"
      },
      "execution_count": 348,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward propagation"
      ],
      "metadata": {
        "id": "pnulycSKMLB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(h, layers):\n",
        "  m = len(layers)\n",
        "  \n",
        "  layers[0].a = np.dot(layers[0].W, h)\n",
        "  layers[0].h = layers[0].act(layers[0].a)\n",
        "  #print(layers[0].h.shape)\n",
        "  \n",
        "  for j in range(1, m-1):\n",
        "    layers[j].a = np.dot(layers[j].W, layers[j-1].h)\n",
        "    layers[j].h = layers[j].act(layers[j].a)\n",
        "    #print(layers[j].h.shape)\n",
        "\n",
        "  j+=1\n",
        "  layers[j].a = np.dot(layers[j].W, layers[j-1].h)\n",
        "  layers[j].h = softmax(layers[j].a)\n",
        "  #print(layers[j].h.shape)\n",
        "\n",
        "  return layers[m-1].h"
      ],
      "metadata": {
        "id": "TK_UOVYHs3Vv"
      },
      "execution_count": 349,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Backward_propagation"
      ],
      "metadata": {
        "id": "9J1L5aJ3MNvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation(l, y_hat, layers, inp):\n",
        "  \n",
        "  #one-hot vector\n",
        "  e_l = np.zeros((y_hat.shape[0], 1))\n",
        "  e_l[l] = 1\n",
        "  \n",
        "  layers[len(layers)-1].da = -(e_l - y_hat)                 #gradient w.r.t activation of last layer (a_L)\n",
        "  \n",
        "  for j in range(len(layers)-1, 0, -1):\n",
        "                        \n",
        "    layers[j].dW += np.dot(layers[j].da, (layers[j-1].h).T)\n",
        "    layers[j].db += layers[j].da\n",
        "\n",
        "    layers[j-1].dh = np.dot((layers[j].W).T, layers[j].da)\n",
        "    layers[j-1].da = np.multiply(layers[j-1].dh, layers[j-1].d_act(layers[j-1].a))\n",
        "\n",
        "  layers[0].dW += np.dot(layers[0].da, inp.T)\n",
        "  layers[0].db += layers[0].da\n",
        "\n",
        "  return layers\n"
      ],
      "metadata": {
        "id": "Ib1ZyluaRYnz"
      },
      "execution_count": 350,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Gradient Descent"
      ],
      "metadata": {
        "id": "wSR_D3vzMR9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_params(learning_rate, layers, batch_size):\n",
        "  for layer in layers:\n",
        "    layer.W = layer.W - learning_rate * layer.dW/batch_size\n",
        "    layer.b = layer.b - learning_rate * layer.db/batch_size\n",
        "\n",
        "    layer.dW = 0\n",
        "    layer.db = 0"
      ],
      "metadata": {
        "id": "7ww8nMm7iVt8"
      },
      "execution_count": 351,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SGD / Batch Gradient Descent"
      ],
      "metadata": {
        "id": "h0Lo597P3hI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(epochs, layers, learning_rate, x_train, y_train, batch_size):\n",
        "    \n",
        "    m = x_train.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      cost = 0\n",
        "\n",
        "      for i in range(m):\n",
        "\n",
        "        inp = x_train[i].reshape(784, 1)\n",
        "        \n",
        "        # Feedforward\n",
        "        h = inp\n",
        "        h = forward_propagation(h, layers)\n",
        "\n",
        "        # Calulate cost to plot graph\n",
        "        cost += cross_entropy_loss(y_train, h, i)\n",
        "\n",
        "        # Backpropagation\n",
        "        backward_propagation(y_train[i], h, layers, x_train[i].reshape(784, 1))\n",
        "\n",
        "        #stocastic gradient decent\n",
        "        if (i+1) % batch_size == 0:\n",
        "          update_params(learning_rate, layers, batch_size)\n",
        "\n",
        "      costs.append(cost/m)\n",
        "\n",
        "      print(\"Cost after epoch \" + str(epoch) + \" :\", cost/m)\n",
        "\n",
        "    return costs, layers"
      ],
      "metadata": {
        "id": "Acn4nh9X3gIQ"
      },
      "execution_count": 352,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Momentum Gradient descent"
      ],
      "metadata": {
        "id": "ZcWfy6JBDyJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mgd(epochs, layers, learning_rate, x_train, y_train, batch_size):\n",
        "\n",
        "    gamma = 0.9\n",
        "    m = x_train.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      for layer in layers:\n",
        "        layer.update_W = 0\n",
        "        layer.update_b = 0\n",
        "\n",
        "      cost = 0\n",
        "\n",
        "      for i in range(m):\n",
        "\n",
        "        inp = x_train[i].reshape(784, 1)\n",
        "        \n",
        "        # Feedforward\n",
        "        h = inp\n",
        "        h = forward_propagation(h, layers)\n",
        "\n",
        "        # Calulate cost to plot graph\n",
        "        cost += cross_entropy_loss(y_train, h, i)\n",
        "\n",
        "        # Backpropagation\n",
        "        backward_propagation(y_train[i], h, layers, x_train[i].reshape(784, 1))\n",
        "\n",
        "        #momentum gradient decent\n",
        "        if (i+1) % batch_size == 0:\n",
        "          for layer in layers:\n",
        "\n",
        "            layer.update_W = gamma*layer.update_W + learning_rate*layer.dW/batch_size\n",
        "            layer.update_b = gamma*layer.update_b + learning_rate*layer.dW/batch_size\n",
        "\n",
        "            layer.W = layer.W - layer.update_W\n",
        "            layer.b = layer.b - layer.update_b\n",
        "\n",
        "            layer.dW = 0\n",
        "            layer.db = 0\n",
        "\n",
        "            layer.update_W = 0\n",
        "            layer.update_b = 0\n",
        "\n",
        "\n",
        "      costs.append(cost/m)\n",
        "\n",
        "      print(\"Cost after epoch \" + str(epoch) + \" :\", cost/m)\n",
        "\n",
        "    return costs, layers"
      ],
      "metadata": {
        "id": "vksSBdq-D1cC"
      },
      "execution_count": 353,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Nesterov Gradient Descent"
      ],
      "metadata": {
        "id": "Lp6A3a8gnoSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nesterov(epochs, layers, learning_rate, x_train, y_train, batch_size):\n",
        "\n",
        "    gamma = 0.9\n",
        "    m = x_train.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      for layer in layers:\n",
        "        layer.update_W = 0\n",
        "        layer.update_b = 0\n",
        "\n",
        "      cost = 0\n",
        "\n",
        "      for i in range(m):\n",
        "\n",
        "        inp = x_train[i].reshape(784, 1)\n",
        "        \n",
        "        # Feedforward\n",
        "        h = inp\n",
        "        h = forward_propagation(h, layers)\n",
        "\n",
        "        # Calulate cost to plot graph\n",
        "        cost += cross_entropy_loss(y_train, h, i)\n",
        "\n",
        "        #calculate W_lookaheads\n",
        "        if (i+1) % batch_size == 0:\n",
        "          for layer in layers:\n",
        "            layer.W = layer.W - gamma * layer.update_W\n",
        "            layer.b = layer.b - gamma * layer.update_b\n",
        "\n",
        "        # Backpropagation\n",
        "        backward_propagation(y_train[i], h, layers, x_train[i].reshape(784, 1))\n",
        "\n",
        "        #nesterov gradient decent\n",
        "        if (i+1) % batch_size == 0:\n",
        "          for layer in layers:\n",
        "\n",
        "            layer.update_W = gamma*layer.update_W + learning_rate*layer.dW/batch_size\n",
        "            layer.update_b = gamma*layer.update_b + learning_rate*layer.dW/batch_size\n",
        "\n",
        "            layer.W = layer.W - layer.update_W\n",
        "            layer.b = layer.b - layer.update_b\n",
        "\n",
        "            layer.dW = 0\n",
        "            layer.db = 0\n",
        "\n",
        "            layer.update_W = 0\n",
        "            layer.update_b = 0\n",
        "\n",
        "      costs.append(cost/m)\n",
        "      print(\"Cost after epoch \" + str(epoch) + \" :\", cost/m)\n",
        "      \n",
        "    return costs, layers  "
      ],
      "metadata": {
        "id": "dBdvwjl3nljy"
      },
      "execution_count": 354,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###RMSProp"
      ],
      "metadata": {
        "id": "pG_GGFWjXaRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsprop(epochs, layers, learning_rate, x_train, y_train, batch_size):\n",
        "\n",
        "    epsilon, beta = 1e-8, 0.9\n",
        "    m = x_train.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      for layer in layers:\n",
        "        layer.update_W = 0\n",
        "        layer.update_b = 0\n",
        "\n",
        "      cost = 0\n",
        "\n",
        "      for i in range(m):\n",
        "\n",
        "        inp = x_train[i].reshape(784, 1)\n",
        "        \n",
        "        # Feedforward\n",
        "        h = inp\n",
        "        h = forward_propagation(h, layers)\n",
        "\n",
        "        # Calulate cost to plot graph\n",
        "        cost += cross_entropy_loss(y_train, h, i)\n",
        "\n",
        "        # Backpropagation\n",
        "        backward_propagation(y_train[i], h, layers, x_train[i].reshape(784, 1))\n",
        "\n",
        "        #rmsprop gradient decent\n",
        "        if (i+1) % batch_size == 0:\n",
        "          for layer in layers:\n",
        "\n",
        "            layer.update_W = beta*layer.update_W + (1-beta)*(layer.dW/batch_size)**2\n",
        "            layer.update_b = beta*layer.update_b + (1-beta)*(layer.db/batch_size)**2\n",
        "\n",
        "            layer.W = layer.W - (learning_rate / np.sqrt(layer.update_W + epsilon)) * (layer.dW/batch_size)\n",
        "            layer.b = layer.b - (learning_rate / np.sqrt(layer.update_b + epsilon)) * (layer.db/batch_size)\n",
        "\n",
        "            layer.dW = 0\n",
        "            layer.db = 0\n",
        "\n",
        "            layer.update_W = 0\n",
        "            layer.update_b = 0\n",
        "\n",
        "\n",
        "      costs.append(cost/m)\n",
        "\n",
        "      print(\"Cost after epoch \" + str(epoch) + \" :\", cost/m)\n",
        "\n",
        "    return costs, layers"
      ],
      "metadata": {
        "id": "uGBnrjWeXcpt"
      },
      "execution_count": 355,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Adam"
      ],
      "metadata": {
        "id": "v0Dgl76nu7Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adam(epochs, layers, learning_rate, x_train, y_train, batch_size):\n",
        "\n",
        "    epsilon, beta1, beta2 = 1e-8, 0.9, 0.99\n",
        "    t = 0\n",
        "    \n",
        "    m = x_train.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      for layer in layers:\n",
        "        layer.m_W, layer.m_b, layer.v_W, layer.v_b, layer.m_W_hat, layer.m_b_hat, layer.v_W_hat, layer.v_b_hat = 0, 0, 0, 0, 0, 0, 0, 0\n",
        "\n",
        "      cost = 0\n",
        "\n",
        "      for i in range(m):\n",
        "\n",
        "        inp = x_train[i].reshape(784, 1)\n",
        "        \n",
        "        # Feedforward\n",
        "        h = inp\n",
        "        h = forward_propagation(h, layers)\n",
        "\n",
        "        # Calulate cost to plot graph\n",
        "        cost += cross_entropy_loss(y_train, h, i)\n",
        "\n",
        "        # Backpropagation\n",
        "        backward_propagation(y_train[i], h, layers, x_train[i].reshape(784, 1))\n",
        "\n",
        "        #adam gradient decent\n",
        "        if (i+1) % batch_size == 0:\n",
        "          t+=1\n",
        "\n",
        "          for layer in layers:\n",
        "\n",
        "            layer.m_W = beta1 * layer.m_W + (1-beta1)*layer.dW/batch_size\n",
        "            layer.m_b = beta1 * layer.m_b + (1-beta1)*layer.db/batch_size\n",
        "\n",
        "            layer.v_W = beta2 * layer.v_W + (1-beta2)*((layer.dW/batch_size))**2\n",
        "            layer.v_b = beta2 * layer.v_b + (1-beta2)*((layer.db/batch_size))**2\n",
        "\n",
        "            layer.m_W_hat = layer.m_W/(1-math.pow(beta1, t))\n",
        "            layer.m_b_hat = layer.m_b/(1-math.pow(beta1, t))\n",
        "\n",
        "            layer.v_W_hat = layer.v_W/(1-math.pow(beta2, t))\n",
        "            layer.v_b_hat = layer.v_b/(1-math.pow(beta2, t))\n",
        "\n",
        "            layer.W = layer.W - (learning_rate/np.sqrt(layer.v_W_hat + epsilon))*layer.m_W_hat\n",
        "            layer.b = layer.b - (learning_rate/np.sqrt(layer.v_b_hat + epsilon))*layer.m_b_hat\n",
        "\n",
        "            layer.dW = 0\n",
        "            layer.db = 0\n",
        "\n",
        "            layer.m_W, layer.m_b, layer.v_W, layer.v_b, layer.m_W_hat, layer.m_b_hat, layer.v_W_hat, layer.v_b_hat = 0, 0, 0, 0, 0, 0, 0, 0\n",
        "\n",
        "\n",
        "      costs.append(cost/m)\n",
        "\n",
        "      print(\"Cost after epoch \" + str(epoch) + \" :\", cost/m)\n",
        "\n",
        "    return costs, layers"
      ],
      "metadata": {
        "id": "OKZU2d9Su9Vk"
      },
      "execution_count": 356,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Putting all togather:"
      ],
      "metadata": {
        "id": "os37IrNr-ClW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Train Model"
      ],
      "metadata": {
        "id": "BN3FKWWGNkN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_train(epochs, learning_rate, x_train, y_train, activation, h_layers, neurons, batch_size = 16):\n",
        "\n",
        "  layers= [Layer(x_train.shape[1], neurons, activation)]\n",
        "\n",
        "  for _ in range(0, h_layers-1):\n",
        "    layers.append(Layer(neurons, neurons, activation))\n",
        "  layers.append(Layer(neurons, 10, 'softmax'))\n",
        "  \n",
        "  #return sgd(epochs, layers, learning_rate, x_train, y_train, batch_size)\n",
        "  #return mgd(epochs, layers, learning_rate, x_train, y_train, batch_size)\n",
        "  #return nesterov(epochs, layers, learning_rate, x_train, y_train, batch_size)\n",
        "  return rmsprop(epochs, layers, learning_rate, x_train, y_train, batch_size)\n",
        "  #return adam(epochs, layers, learning_rate, x_train, y_train, batch_size)"
      ],
      "metadata": {
        "id": "3sRz9nk6NgVa"
      },
      "execution_count": 357,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import dataset and putting in appropriate format"
      ],
      "metadata": {
        "id": "VcVmssSHNp78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(x_train_org, y_train_org), (x_test_org, y_test_org) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "-gFb9PXW50Fc"
      },
      "execution_count": 358,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"x_train shape: \", x_train_org.shape)\n",
        "print(\"y_train shape: \", y_train_org.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxKqgiBfFg0R",
        "outputId": "59f02013-848b-479a-eb1b-c134a8243f77"
      },
      "execution_count": 359,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape:  (60000, 28, 28)\n",
            "y_train shape:  (60000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train_org.reshape(x_train_org.shape[0], -1)\n",
        "y_train = y_train_org"
      ],
      "metadata": {
        "id": "aVrG8aF6A6HY"
      },
      "execution_count": 360,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"x_train shape: \", x_train.shape)\n",
        "print(\"y_train shape: \", y_train.shape)\n",
        "print((x_train[0].reshape(784, 1)).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQzSXM6cJn2H",
        "outputId": "29d1f659-dae5-44a9-e914-01edd0eb981a"
      },
      "execution_count": 361,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape:  (60000, 784)\n",
            "y_train shape:  (60000,)\n",
            "(784, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train Model"
      ],
      "metadata": {
        "id": "Wfm6cD7aM9nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "learning_rate = 0.001\n",
        "activation = 'sigmoid'\n",
        "h_layers = 5\n",
        "neurons = 32\n",
        "\n",
        "costs, layers = model_train(epochs, learning_rate, x_train, y_train, activation, h_layers, neurons)"
      ],
      "metadata": {
        "id": "hrKrKnZSKxYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e1f2fbe-f02c-4f32-d769-99d1a6749f50"
      },
      "execution_count": 362,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after epoch 0 : 1.1260610293644548\n",
            "Cost after epoch 1 : 0.9698389073562047\n",
            "Cost after epoch 2 : 0.9772540739709472\n",
            "Cost after epoch 3 : 1.009371061988453\n",
            "Cost after epoch 4 : 1.0476266612552678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions\n",
        "h = forward_propagation(x_train.T, layers)\n",
        "print(h)"
      ],
      "metadata": {
        "id": "cV2-vMJ4igo4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2d69621-71ae-4b5e-b007-dd89455ab63c"
      },
      "execution_count": 363,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.68032599e-12 9.86997195e-16 2.43824350e-17 ... 1.61339879e-15\n",
            "  4.84457208e-12 1.22094847e-09]\n",
            " [6.24327526e-13 1.07435666e-17 1.08284251e-12 ... 6.81591112e-17\n",
            "  1.43516736e-15 1.71312009e-11]\n",
            " [1.99043283e-10 2.59964325e-17 3.50783454e-16 ... 2.47165635e-16\n",
            "  1.76200236e-12 1.28776413e-06]\n",
            " ...\n",
            " [1.41972924e-08 7.81365828e-21 7.83488208e-18 ... 5.08996833e-19\n",
            "  1.52707069e-15 1.18366340e-06]\n",
            " [2.99944637e-10 4.97076216e-21 2.64876619e-20 ... 1.47760090e-18\n",
            "  1.95501881e-15 1.38949561e-06]\n",
            " [1.57569518e-07 2.57832717e-20 8.75870055e-20 ... 1.96374552e-18\n",
            "  7.24004805e-15 1.30818935e-07]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(epochs), costs)\n",
        "print(costs)"
      ],
      "metadata": {
        "id": "fioKXqPcivBs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "4350ae62-f28d-45c5-ade2-432b094b0130"
      },
      "execution_count": 364,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.1260610293644548, 0.9698389073562047, 0.9772540739709472, 1.009371061988453, 1.0476266612552678]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in layers:\n",
        "  print(layer.W.shape, layer.b.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt59AZYbSvlA",
        "outputId": "297895a0-e682-4c56-e647-e1cd594b4fe6"
      },
      "execution_count": 365,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 784) (32, 1)\n",
            "(32, 32) (32, 1)\n",
            "(32, 32) (32, 1)\n",
            "(32, 32) (32, 1)\n",
            "(32, 32) (32, 1)\n",
            "(10, 32) (10, 1)\n"
          ]
        }
      ]
    }
  ]
}
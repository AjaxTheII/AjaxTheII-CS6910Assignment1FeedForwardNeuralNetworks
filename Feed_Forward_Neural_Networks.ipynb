{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feed_Forward_Neural_Networks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "fAgsLmKZMpyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Activation Functions"
      ],
      "metadata": {
        "id": "Mt8dmitqMs_4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lE5PJPWMcKJ"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1 + np.exp(-x))\n",
        "\n",
        "def d_sigmoid(x):\n",
        "  return (1 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1 - np.square(np.tanh(x))\n",
        "\n",
        "def relu(x):\n",
        "  return np.where(np.asarray(x) > 0, x, 0)\n",
        "\n",
        "def d_relu(x):\n",
        "    return np.where(x <= 0, 0, 1)\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x)\n",
        "    return e_x/e_x.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cost Function:"
      ],
      "metadata": {
        "id": "Yq4IZu4f2hCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(y, y_hat, i):\n",
        "  return -np.log(y_hat[y[i]][0])"
      ],
      "metadata": {
        "id": "2EE-A_Pw2kMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cost(y, y_hat, i):\n",
        "  \n",
        "  m = y.shape[0]\n",
        "  c = (1/m) * np.sum(cross_entropy_loss(y, y_hat))\n",
        "  c = np.squeeze(c) \n",
        "\n",
        "  return c"
      ],
      "metadata": {
        "id": "qJzVsQoi2yek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Layer Class : parameters initialization for each layer"
      ],
      "metadata": {
        "id": "x23K52DJMwJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "\n",
        "    activationFunc = {\n",
        "        'tanh': (tanh, d_tanh),\n",
        "        'sigmoid': (sigmoid, d_sigmoid),\n",
        "        'relu' : (relu, d_relu),\n",
        "        'softmax' : (softmax, None)\n",
        "    }\n",
        "\n",
        "    def __init__(self, inputs, neurons, activation):\n",
        "        np.random.seed(44)\n",
        "        self.W = np.random.randn(neurons, inputs)\n",
        "        self.b = np.zeros((neurons, 1))\n",
        "        self.act, self.d_act = self.activationFunc.get(activation)\n",
        "        self.dW = 0\n",
        "        self.db = 0"
      ],
      "metadata": {
        "id": "MSYeILJNMvIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward propagation"
      ],
      "metadata": {
        "id": "pnulycSKMLB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(h, layers):\n",
        "  m = len(layers)\n",
        "  \n",
        "  layers[0].a = np.dot(layers[0].W, h)\n",
        "  layers[0].h = layers[0].act(layers[0].a)\n",
        "  #print(layers[0].h.shape)\n",
        "  \n",
        "  for j in range(1, m-1):\n",
        "    layers[j].a = np.dot(layers[j].W, layers[j-1].h)\n",
        "    layers[j].h = layers[j].act(layers[j].a)\n",
        "    #print(layers[j].h.shape)\n",
        "\n",
        "  j+=1\n",
        "  layers[j].a = np.dot(layers[j].W, layers[j-1].h)\n",
        "  layers[j].h = softmax(layers[j].a)\n",
        "  #print(layers[j].h.shape)\n",
        "\n",
        "  return layers[m-1].h"
      ],
      "metadata": {
        "id": "TK_UOVYHs3Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Backward_propagation"
      ],
      "metadata": {
        "id": "9J1L5aJ3MNvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation(l, y_hat, layers, inp):\n",
        "  \n",
        "  #one-hot vector\n",
        "  e_l = np.zeros((y_hat.shape[0], 1))\n",
        "  e_l[l] = 1\n",
        "  \n",
        "  layers[len(layers)-1].da = -(e_l - y_hat)                 #gradient w.r.t activation of last layer (a_L)\n",
        "  \n",
        "  for j in range(len(layers)-1, 0, -1):\n",
        "                        \n",
        "    layers[j].dW += np.dot(layers[j].da, (layers[j-1].h).T)\n",
        "    layers[j].db += layers[j].da\n",
        "\n",
        "    layers[j-1].dh = np.dot((layers[j].W).T, layers[j].da)\n",
        "    layers[j-1].da = np.multiply(layers[j-1].dh, layers[j-1].d_act(layers[j-1].a))\n",
        "\n",
        "  layers[0].dW += np.dot(layers[0].da, inp.T)\n",
        "  layers[0].db += layers[0].da\n",
        "\n",
        "  return layers\n"
      ],
      "metadata": {
        "id": "Ib1ZyluaRYnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Gradient Descent"
      ],
      "metadata": {
        "id": "wSR_D3vzMR9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_params(learning_rate, layers, batch_size):\n",
        "  for layer in layers:\n",
        "    layer.W = layer.W - learning_rate * layer.dW/batch_size\n",
        "    layer.b = layer.b - learning_rate * layer.db/batch_size\n",
        "\n",
        "    layer.dW = 0\n",
        "    layer.db = 0"
      ],
      "metadata": {
        "id": "7ww8nMm7iVt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SGD / Batch Gradient Descent"
      ],
      "metadata": {
        "id": "h0Lo597P3hI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(epochs, layers, learning_rate, x_train, y_train, batch_size):\n",
        "    \n",
        "    m = x_train.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      cost = 0\n",
        "\n",
        "      for i in range(m):\n",
        "\n",
        "        inp = x_train[i].reshape(784, 1)\n",
        "        \n",
        "        # Feedforward\n",
        "        h = inp\n",
        "        h = forward_propagation(h, layers)\n",
        "\n",
        "        # Calulate cost to plot graph\n",
        "        cost += cross_entropy_loss(y_train, h, i)\n",
        "\n",
        "        # Backpropagation\n",
        "        backward_propagation(y_train[i], h, layers, x_train[i].reshape(784, 1))\n",
        "\n",
        "        #stocastic gradient decent\n",
        "        if (i+1) % batch_size == 0:\n",
        "          update_params(learning_rate, layers, batch_size)\n",
        "\n",
        "      costs.append(cost/m)\n",
        "\n",
        "      print(\"Cost after epoch \" + str(epoch) + \" :\", cost/m)\n",
        "\n",
        "    return costs, layers"
      ],
      "metadata": {
        "id": "Acn4nh9X3gIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Momentum Gradient descent"
      ],
      "metadata": {
        "id": "ZcWfy6JBDyJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mgd(epochs, layers, learning_rate, x_train, y_train, batch_size):\n",
        "\n",
        "    gamma = 0.9\n",
        "    m = x_train.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      for layer in layers:\n",
        "        layer.update_W = 0\n",
        "        layer.update_b = 0\n",
        "\n",
        "      cost = 0\n",
        "\n",
        "      for i in range(m):\n",
        "\n",
        "        inp = x_train[i].reshape(784, 1)\n",
        "        \n",
        "        # Feedforward\n",
        "        h = inp\n",
        "        h = forward_propagation(h, layers)\n",
        "\n",
        "        # Calulate cost to plot graph\n",
        "        cost += cross_entropy_loss(y_train, h, i)\n",
        "\n",
        "        # Backpropagation\n",
        "        backward_propagation(y_train[i], h, layers, x_train[i].reshape(784, 1))\n",
        "\n",
        "      #momentum gradient decent\n",
        "      if (i+1) % batch_size == 0:\n",
        "        for layer in layers:\n",
        "\n",
        "          layer.update_W = gamma*layer.update_W + learning_rate*layer.dW/batch_size\n",
        "          layer.update_b = gamma*layer.update_b + learning_rate*layer.dW/batch_size\n",
        "\n",
        "          layer.W = layer.W - layer.update_W\n",
        "          layer.b = layer.b - layer.update_b\n",
        "\n",
        "          layer.dW = 0\n",
        "          layer.db = 0\n",
        "\n",
        "          layer.update_W = 0\n",
        "          layer.update_b = 0\n",
        "\n",
        "\n",
        "      costs.append(cost/m)\n",
        "\n",
        "      print(\"Cost after epoch \" + str(epoch) + \" :\", cost/m)\n",
        "\n",
        "    return costs, layers"
      ],
      "metadata": {
        "id": "vksSBdq-D1cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Nesterov Gradient Descent"
      ],
      "metadata": {
        "id": "Lp6A3a8gnoSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nesterov(epochs, layers, learning_rate, x_train, y_train, batch_size):\n",
        "\n",
        "    gamma = 0.9\n",
        "    m = x_train.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      for layer in layers:\n",
        "        layer.update_W = 0\n",
        "        layer.update_b = 0\n",
        "\n",
        "      cost = 0\n",
        "\n",
        "      for i in range(m):\n",
        "\n",
        "        inp = x_train[i].reshape(784, 1)\n",
        "        \n",
        "        # Feedforward\n",
        "        h = inp\n",
        "        h = forward_propagation(h, layers)\n",
        "\n",
        "        # Calulate cost to plot graph\n",
        "        cost += cross_entropy_loss(y_train, h, i)\n",
        "\n",
        "        #calculate W_lookaheads\n",
        "        if (i+1) % batch_size == 0:\n",
        "          for layer in layers:\n",
        "            layer.W = layer.W - gamma * layer.update_W\n",
        "            layer.b = layer.b - gamma * layer.update_b\n",
        "\n",
        "        # Backpropagation\n",
        "        backward_propagation(y_train[i], h, layers, x_train[i].reshape(784, 1))\n",
        "\n",
        "        #nesterov gradient decent\n",
        "        if (i+1) % batch_size == 0:\n",
        "          for layer in layers:\n",
        "\n",
        "            layer.update_W = gamma*layer.update_W + learning_rate*layer.dW/batch_size\n",
        "            layer.update_b = gamma*layer.update_b + learning_rate*layer.dW/batch_size\n",
        "\n",
        "            layer.W = layer.W - layer.update_W\n",
        "            layer.b = layer.b - layer.update_b\n",
        "\n",
        "            layer.dW = 0\n",
        "            layer.db = 0\n",
        "\n",
        "            layer.update_W = 0\n",
        "            layer.update_b = 0\n",
        "\n",
        "      costs.append(cost/m)\n",
        "      print(\"Cost after epoch \" + str(epoch) + \" :\", cost/m)\n",
        "      \n",
        "    return costs, layers  "
      ],
      "metadata": {
        "id": "dBdvwjl3nljy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Putting all togather:"
      ],
      "metadata": {
        "id": "os37IrNr-ClW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Train Model"
      ],
      "metadata": {
        "id": "BN3FKWWGNkN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_train(epochs, learning_rate, x_train, y_train, activation, h_layers, neurons, batch_size = 16):\n",
        "\n",
        "  layers= [Layer(x_train.shape[1], neurons, activation)]\n",
        "\n",
        "  for _ in range(0, h_layers-1):\n",
        "    layers.append(Layer(neurons, neurons, activation))\n",
        "  layers.append(Layer(neurons, 10, 'softmax'))\n",
        "  \n",
        "  #return sgd(epochs, layers, learning_rate, x_train, y_train, batch_size)\n",
        "  #return mgd(epochs, layers, learning_rate, x_train, y_train, batch_size)\n",
        "  return nesterov(epochs, layers, learning_rate, x_train, y_train, batch_size)\n",
        "  "
      ],
      "metadata": {
        "id": "3sRz9nk6NgVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import dataset and putting in appropriate format"
      ],
      "metadata": {
        "id": "VcVmssSHNp78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(x_train_org, y_train_org), (x_test_org, y_test_org) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "-gFb9PXW50Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"x_train shape: \", x_train_org.shape)\n",
        "print(\"y_train shape: \", y_train_org.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxKqgiBfFg0R",
        "outputId": "373faad3-66e5-42e0-edfe-25187e62cdbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape:  (60000, 28, 28)\n",
            "y_train shape:  (60000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train_org.reshape(x_train_org.shape[0], -1)\n",
        "y_train = y_train_org"
      ],
      "metadata": {
        "id": "aVrG8aF6A6HY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"x_train shape: \", x_train.shape)\n",
        "print(\"y_train shape: \", y_train.shape)\n",
        "print((x_train[0].reshape(784, 1)).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQzSXM6cJn2H",
        "outputId": "c3443618-7378-4b07-eed4-cf9166acf10a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape:  (60000, 784)\n",
            "y_train shape:  (60000,)\n",
            "(784, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train Model"
      ],
      "metadata": {
        "id": "Wfm6cD7aM9nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "learning_rate = 0.001\n",
        "activation = 'sigmoid'\n",
        "h_layers = 5\n",
        "neurons = 32\n",
        "\n",
        "costs, layers = model_train(epochs, learning_rate, x_train, y_train, activation, h_layers, neurons)"
      ],
      "metadata": {
        "id": "hrKrKnZSKxYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76772d00-fc9d-46a3-869f-ebb3649a1e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after epoch 0 : 2.55810798816512\n",
            "Cost after epoch 1 : 2.1763730730567965\n",
            "Cost after epoch 2 : 2.071601685700959\n",
            "Cost after epoch 3 : 1.9714907008394138\n",
            "Cost after epoch 4 : 1.8680626626848127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions\n",
        "h = forward_propagation(x_train.T, layers)\n",
        "print(h)"
      ],
      "metadata": {
        "id": "cV2-vMJ4igo4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df7f0303-14d7-42d5-c81f-d04acf4e2af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.56877889e-06 2.39141423e-06 1.70712270e-06 ... 6.53665405e-07\n",
            "  1.14678318e-06 4.73103423e-06]\n",
            " [4.69308461e-07 3.49474435e-07 1.69101223e-05 ... 1.46314330e-06\n",
            "  1.66258295e-07 1.84724743e-07]\n",
            " [1.32530196e-06 1.89250222e-06 2.62373016e-07 ... 1.23143081e-06\n",
            "  2.15116478e-06 1.06087601e-06]\n",
            " ...\n",
            " [2.49872619e-06 1.47797918e-06 7.83756079e-07 ... 1.01475603e-06\n",
            "  1.40847654e-06 1.69954960e-06]\n",
            " [2.65463599e-06 1.96701855e-06 6.82950544e-07 ... 6.47277051e-07\n",
            "  1.14195519e-06 1.22601019e-06]\n",
            " [2.77824873e-06 1.34248974e-06 6.92831739e-07 ... 6.64396215e-07\n",
            "  1.63920675e-06 1.68662083e-06]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(epochs), costs)\n",
        "print(costs)"
      ],
      "metadata": {
        "id": "fioKXqPcivBs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "9d6a6062-38fc-4074-a1b8-71cbba19740d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.55810798816512, 2.1763730730567965, 2.071601685700959, 1.9714907008394138, 1.8680626626848127]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in layers:\n",
        "  print(layer.W.shape, layer.b.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt59AZYbSvlA",
        "outputId": "978f2c34-2c57-41b7-ba9b-7881a6af1a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 784) (32, 784)\n",
            "(32, 32) (32, 32)\n",
            "(32, 32) (32, 32)\n",
            "(32, 32) (32, 32)\n",
            "(32, 32) (32, 32)\n",
            "(10, 32) (10, 32)\n"
          ]
        }
      ]
    }
  ]
}